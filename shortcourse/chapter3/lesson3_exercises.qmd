---
title: >
  Lesson 3: Exercises on Likelihood-based Inference for POMP Models
author:
  - Aaron A. King
  - Edward L. Ionides
  - Translated in pypomp by Kunyang He

shortauthor: "King & Ionides et al."   
shorttitle: "Lesson 3 Exercises"  
date: "December 23, 2025"

bibliography: ../sbied.bib             

format:
  beamer:
    code-block-font-size: \tiny                    
    theme: AnnArbor
    colortheme: default
    fontsize: 11pt
    cite-method: natbib
    biblio-style: apalike
    toc: true  
    slide-level: 3
    highlight-style: tango  
  
  pdf:
    documentclass: article          
    fontsize: 11pt
    cite-method: natbib           
    biblio-style: apalike
    toc: true
    geometry: margin=1in
    
jupyter: python3 


header-includes: |
  \providecommand{\AtBeginSection}[1]{}
  \providecommand{\AtBeginSubsection}[1]{}
  \providecommand{\framebreak}{}

  \usepackage{fvextra}  
  \RecustomVerbatimEnvironment{Highlighting}{Verbatim}{
    commandchars=\\\{\}, 
    fontsize=\scriptsize 
  }
  \AtBeginSection{}
  \AtBeginSubsection{}
  \usepackage{amsmath,amssymb,amsfonts}
  \usepackage{graphicx}
  \usepackage{hyperref}
  \usepackage{xcolor}

  \newcommand{\myemph}[1]{\emph{#1}}
  \newcommand{\deriv}[2]{\frac{d #1}{d #2}}
  \newcommand{\pkg}[1]{\texttt{#1}}
  \newcommand{\code}[1]{\texttt{#1}}
  \newcommand{\Rlanguage}{\textsf{R}}
  \newcommand{\Rzero}{\mathcal{R}_{0}}
  \newcommand{\pr}{\mathbb{P}}
  \newcommand{\E}{\mathbb{E}}
  \newcommand{\lik}{\mathcal{L}}
  \newcommand{\loglik}{\ell}
  \newcommand{\equals}{=}
  \newcommand{\dist}[2]{\operatorname{#1}\!\bigl(#2\bigr)}
  \newcommand{\myexercise}{\paragraph{Exercise}}
---

This document contains worked solutions to the exercises from Lesson 3 on 
likelihood-based inference for POMP models, implemented using `pypomp`.

# Setup

## Import Required Packages

```{python}
#| echo: true
import jax.numpy as jnp
import jax
import pandas as pd
import numpy as np
import pypomp as pp
import matplotlib.pyplot as plt
import time
from scipy import stats
```

## Load Data and Build Model

```{python}
#| echo: true
# Download and prepare data
meas = (pd.read_csv(
          "https://kingaa.github.io/sbied/stochsim/Measles_Consett_1948.csv")
          .loc[:, ["week", "cases"]]
          .rename(columns={"week": "time", "cases": "reports"})
          .set_index("time")
          .astype(float))

ys = meas.copy()
ys.columns = pd.Index(["reports"])
```

## Helper Functions

```{python}
#| echo: true
def nbinom_logpmf(x, k, mu):
    """Log PMF of NegBin(k, mu) that is robust when mu == 0."""
    x = jnp.asarray(x)
    k = jnp.asarray(k)
    mu = jnp.asarray(mu)
    logp_zero = jnp.where(x == 0, 0.0, -jnp.inf)
    safe_mu = jnp.where(mu == 0.0, 1.0, mu)
    core = (jax.scipy.special.gammaln(k + x) 
            - jax.scipy.special.gammaln(k)
            - jax.scipy.special.gammaln(x + 1)
            + k * jnp.log(k / (k + safe_mu))
            + x * jnp.log(safe_mu / (k + safe_mu)))
    return jnp.where(mu == 0.0, logp_zero, core)

def rnbinom(key, k, mu):
    """Sample from NegBin(k, mu) via Gamma-Poisson mixture."""
    key_g, key_p = jax.random.split(key)
    lam = jax.random.gamma(key_g, k) * (mu / k)
    return jax.random.poisson(key_p, lam)
```

## SIR Model Components

```{python}
#| echo: true
def rinit(theta_, key, covars, t0):
    """Initial state simulator for SIR model."""
    N = theta_["N"]
    eta = theta_["eta"]
    S0 = jnp.round(N * eta)
    I0 = 1.0
    R0 = jnp.round(N * (1 - eta)) - 1.0   
    H0 = 0.0                           
    return {"S": S0, "I": I0, "R": R0, "H": H0}

def rproc(X_, theta_, key, covars, t, dt):
    """Process simulator for SIR model."""
    S, I, R, H = X_["S"], X_["I"], X_["R"], X_["H"]
    Beta = theta_["Beta"]
    mu_IR = theta_["mu_IR"]
    N = theta_["N"]

    p_SI = 1.0 - jnp.exp(-Beta * I / N * dt)
    p_IR = 1.0 - jnp.exp(-mu_IR * dt)

    key_SI, key_IR = jax.random.split(key)
    dN_SI = jax.random.binomial(key_SI, n=S.astype(jnp.int32), p=p_SI)
    dN_IR = jax.random.binomial(key_IR, n=I.astype(jnp.int32), p=p_IR)

    return {"S": S - dN_SI, "I": I + dN_SI - dN_IR, 
            "R": R + dN_IR, "H": H + dN_IR}

def dmeas(Y_, X_, theta_, covars, t):
    """Measurement density: log P(reports | H, rho, k)."""
    rho = theta_["rho"]
    k = theta_["k"]
    H = X_["H"]
    mu = rho * H
    return nbinom_logpmf(Y_["reports"], k, mu)

def rmeas(X_, theta_, key, covars, t):
    """Measurement simulator."""
    rho = theta_["rho"]
    k = theta_["k"]
    H = X_["H"]
    mu = rho * H
    reports = rnbinom(key, k, mu)
    return jnp.array([reports])
```

## Create POMP Object

```{python}
#| echo: true
theta = {
    "Beta": 15.0,
    "mu_IR": 0.5,
    "N": 38000.0,
    "eta": 0.06,
    "rho": 0.5,
    "k": 10.0
}

statenames = ["S", "I", "R", "H"]

measSIR = pp.Pomp(
    rinit=rinit,
    rproc=rproc,
    dmeas=dmeas,
    rmeas=rmeas,
    ys=ys,
    theta=theta,
    statenames=statenames,
    t0=0.0,
    nstep=7,
    accumvars=(3,),
    ydim=1,
    covars=None
)
```


# Exercise 3.1: Slices and Profiles

## Problem Statement

What is the difference between a likelihood **slice** and a **profile**? What is the consequence of this difference for the statistical interpretation of these plots? How should you decide whether to compute a profile or a slice?

## Solution {.allowframebreaks}

**Likelihood Slice:**

A likelihood slice fixes all parameters except one at specified values and varies only that one parameter. For example, a slice in the $\beta$ direction at our current parameter values:
$$
\loglik_{\text{slice}}(\beta) = \loglik(\beta, \mu_{IR}^0, \eta^0, \rho^0, k^0, N^0)
$$

**Profile Likelihood:**

A profile likelihood **optimizes** over all other parameters for each value of the parameter of interest:
$$
\loglik^{\text{profile}}(\beta) = \max_{\mu_{IR}, \eta, \rho, k, N} \loglik(\beta, \mu_{IR}, \eta, \rho, k, N)
$$

\framebreak

**Consequences for Statistical Interpretation:**

1. **Slices** show the likelihood surface conditional on fixed values of other parameters. They do not account for the fact that other parameters might be adjusted.

2. **Profiles** provide valid confidence intervals via Wilks' theorem. The profile likelihood accounts for uncertainty in nuisance parameters.

3. A slice can **underestimate** how good the likelihood can be at a parameter value, because it doesn't optimize over other parameters.

\framebreak

**When to Use Which:**

| Use Slice When | Use Profile When |
|----------------|------------------|
| Quick exploration | Formal inference |
| Understanding local geometry | Constructing confidence intervals |
| Checking parameter sensitivity | Model comparison |
| Computational resources limited | Results need to be published |

Profiles are computationally much more expensive (require optimization at each point) but are necessary for proper statistical inference.


# Exercise 3.2: Cost of Particle Filter

## Problem Statement

- How much computer processing time does a particle filter take?
- How does this scale with the number of particles?

Form a conjecture based upon your understanding of the algorithm. Test your conjecture by running a sequence of particle filter operations, with increasing numbers of particles ($J$), measuring the time taken for each one. Plot and interpret your results.

## Conjecture {.allowframebreaks}

**Theoretical Analysis:**

The particle filter algorithm consists of:

1. **Initialize**: $O(J)$ operations to initialize $J$ particles
2. **For each time step $n = 1, \ldots, N$:**
   - **Predict**: Simulate each particle forward: $O(J)$
   - **Weight**: Compute weights for each particle: $O(J)$
   - **Resample**: Sample $J$ particles with replacement: $O(J)$

Total complexity: $O(N \times J)$

\framebreak

**Conjecture:** The computational time should scale **linearly** with the number of particles $J$.

With JAX, there may be:
- Some fixed overhead from JIT compilation
- Potentially better-than-linear scaling for small $J$ due to vectorization
- Linear scaling for large $J$

## Testing the Conjecture {.allowframebreaks}

```{python}
#| echo: true
def time_pfilter(pomp_obj, J, n_reps=3):
    """Time how long a particle filter takes."""
    times = []
    for rep in range(n_reps):
        key = jax.random.key(rep + int(J))
        start = time.time()
        pomp_obj.pfilter(key=key, J=J, reps=1)
        # Block until computation is complete (important for JAX timing)
        result = pomp_obj.results_history.last()
        _ = result.logLiks.values  # Force computation
        elapsed = time.time() - start
        times.append(elapsed)
    return np.mean(times), np.std(times)

# Test different numbers of particles
J_values = [100, 500, 1000, 2000, 5000, 10000, 20000, 50000]

timing_results = []
for J in J_values:
    mean_time, std_time = time_pfilter(measSIR, J, n_reps=3)
    timing_results.append({
        'J': J,
        'time_mean': mean_time,
        'time_std': std_time
    })
    print(f"J = {J:6d}: {mean_time:.4f} ± {std_time:.4f} seconds")

timing_df = pd.DataFrame(timing_results)
```

\framebreak

```{python}
#| echo: true
# Plot results
fig, axes = plt.subplots(1, 2, figsize=(8, 3.5))

# Linear scale
ax = axes[0]
ax.errorbar(timing_df['J'], timing_df['time_mean'], 
            yerr=timing_df['time_std'], fmt='o-', capsize=3)
ax.set(xlabel='Number of particles (J)', ylabel='Time (seconds)',
       title='Particle Filter Timing')
ax.grid(alpha=0.3)

# Log-log scale
ax = axes[1]
ax.loglog(timing_df['J'], timing_df['time_mean'], 'o-')
ax.set(xlabel='Number of particles (J)', ylabel='Time (seconds)',
       title='Log-Log Scale')
ax.grid(alpha=0.3, which='both')

plt.tight_layout()
plt.show()
```

\framebreak

```{python}
#| echo: true
# Fit linear model to check scaling
from scipy.stats import linregress

# For larger J values (to avoid overhead effects)
mask = timing_df['J'] >= 1000
slope, intercept, r_value, p_value, std_err = linregress(
    timing_df.loc[mask, 'J'], 
    timing_df.loc[mask, 'time_mean']
)

print(f"Linear fit (J >= 1000):")
print(f"  Time = {intercept:.4f} + {slope:.2e} * J")
print(f"  R² = {r_value**2:.4f}")
print(f"  Time per 1000 particles: {slope * 1000 * 1000:.1f} μs")
```

## Interpretation

The results confirm that:

1. **Linear scaling**: Time scales linearly with the number of particles (as expected from $O(N \times J)$ complexity)

2. **Fixed overhead**: There is a small fixed cost for JIT compilation and setup

3. **Marginal cost**: The marginal cost is approximately constant per particle per time step

4. **Practical implication**: Doubling the number of particles approximately doubles the computation time


# Exercise 3.3: Log-likelihood Estimation

## Problem Statement {.allowframebreaks}

Set up a likelihood evaluation for the measles model, choosing the numbers of particles and replications so that your evaluation takes approximately one minute on your machine.

- Provide a Monte Carlo standard error for your estimate.
- Comment on the bias of your estimate.

## Solution {.allowframebreaks}

First, let's estimate how many replications we can do in one minute:

```{python}
#| echo: true
# Time a single evaluation with moderate J
J_test = 10000
key = jax.random.key(999)

start = time.time()
measSIR.pfilter(key=key, J=J_test, reps=1)
result = measSIR.results_history.last()
_ = result.logLiks.values  # Force computation
test_time = time.time() - start

print(f"Time for J={J_test}: {test_time:.3f} seconds")

# Estimate how many reps in 60 seconds
target_time = 60  # seconds
estimated_reps = int(target_time / test_time)
print(f"Estimated replications in {target_time}s: {estimated_reps}")
```

\framebreak

```{python}
#| echo: true
# Run the likelihood estimation
J = 10000
n_reps = min(50, max(10, estimated_reps))  # Reasonable range

print(f"Running {n_reps} replicates with J={J}...")
start_time = time.time()

key = jax.random.key(1000)
measSIR.pfilter(key=key, J=J, reps=n_reps)

# Get results
result = measSIR.results_history.last()
logliks = result.logLiks.values[0, :]  # All replicates

total_time = time.time() - start_time
print(f"Total time: {total_time:.1f} seconds")
```

\framebreak

```{python}
#| echo: true
# Compute estimates using pypomp's utility functions
ll_array = np.array(logliks)

# Simple mean and SE
mean_ll = np.mean(ll_array)
se_ll = np.std(ll_array, ddof=1) / np.sqrt(n_reps)

# logmeanexp estimate (less biased for likelihood)
lme_ll = pp.logmeanexp(ll_array)
lme_se = pp.logmeanexp_se(ll_array)

print(f"\nResults:")
print(f"  Number of particles: {J}")
print(f"  Number of replications: {n_reps}")
print(f"  Simple mean log-likelihood: {mean_ll:.4f} (SE: {se_ll:.4f})")
print(f"  logmeanexp estimate: {lme_ll:.4f} (SE: {lme_se:.4f})")
```

\framebreak

```{python}
#| echo: true
# Visualize the distribution of log-likelihoods
fig, ax = plt.subplots(figsize=(5, 3.5))
ax.hist(ll_array, bins=15, edgecolor='black', alpha=0.7)
ax.axvline(mean_ll, color='red', linestyle='--', 
           label=f'Mean: {mean_ll:.2f}')
ax.axvline(lme_ll, color='blue', linestyle='-', 
           label=f'logmeanexp: {lme_ll:.2f}')
ax.set(xlabel='Log-likelihood', ylabel='Count',
       title=f'Distribution of log-likelihood estimates (J={J})')
ax.legend()
ax.grid(alpha=0.3)
plt.tight_layout()
plt.show()
```

## Comment on Bias {.allowframebreaks}

**The particle filter provides:**

- An **unbiased** estimate of the likelihood $\lik(\theta)$
- A **biased** estimate of the log-likelihood $\loglik(\theta)$

This is because:
$$
\E[\log \hat{\lik}] \neq \log \E[\hat{\lik}] = \log \lik
$$

By Jensen's inequality, since $\log$ is concave:
$$
\E[\log \hat{\lik}] \leq \log \E[\hat{\lik}]
$$

**The bias is negative** — the simple average of log-likelihoods underestimates the true log-likelihood.

\framebreak

**The `logmeanexp` function helps:**

Instead of computing $\frac{1}{n}\sum_i \log \hat{\lik}_i$, we compute:
$$
\log\left(\frac{1}{n}\sum_i \hat{\lik}_i\right) = \log\left(\frac{1}{n}\sum_i e^{\log \hat{\lik}_i}\right)
$$

This averages the likelihoods (unbiased) and then takes the log, reducing bias.

\framebreak

**Reducing bias:**

1. **Increase particles ($J$)**: More particles → lower variance → less bias
2. **Use logmeanexp**: Average likelihoods instead of log-likelihoods
3. **More replications**: Better estimate of the mean

The difference between `mean_ll` and `lme_ll` in our results illustrates this bias.


# Exercise 3.4: Likelihood Slice in η Direction

## Problem Statement

Compute several likelihood slices in the $\eta$ direction.

## Solution {.allowframebreaks}

```{python}
#| echo: true
def compute_likelihood_slice(param_name, param_values, base_theta, 
                             J=5000, n_reps=3):
    """Compute log-likelihood for a slice of parameter values."""
    results = []
    for val in param_values:
        theta_test = base_theta.copy()
        theta_test[param_name] = val
        
        pomp_test = pp.Pomp(
            rinit=rinit, rproc=rproc, dmeas=dmeas, rmeas=rmeas,
            ys=ys, theta=theta_test, statenames=statenames,
            t0=0.0, nstep=7, accumvars=(3,), ydim=1, covars=None
        )
        
        key = jax.random.key(int(val * 10000))
        pomp_test.pfilter(key=key, J=J, reps=n_reps)
        
        pf_result = pomp_test.results_history.last()
        logliks_arr = pf_result.logLiks.values[0, :]
        
        results.append({
            param_name: val,
            'loglik': pp.logmeanexp(logliks_arr),
            'loglik_se': pp.logmeanexp_se(logliks_arr)
        })
    
    return pd.DataFrame(results)
```

\framebreak

```{python}
#| echo: true
# Compute eta slices at different Beta values
eta_values = np.linspace(0.01, 0.12, 20)

# Slice at current Beta
eta_slice1 = compute_likelihood_slice("eta", eta_values, theta, J=3000, n_reps=3)

# Slice at higher Beta
theta_high_beta = theta.copy()
theta_high_beta["Beta"] = 25.0
eta_slice2 = compute_likelihood_slice("eta", eta_values, theta_high_beta, 
                                       J=3000, n_reps=3)

# Slice at lower Beta
theta_low_beta = theta.copy()
theta_low_beta["Beta"] = 10.0
eta_slice3 = compute_likelihood_slice("eta", eta_values, theta_low_beta,
                                       J=3000, n_reps=3)
```

\framebreak

```{python}
#| echo: true
# Plot all slices
fig, ax = plt.subplots(figsize=(5, 4))

ax.errorbar(eta_slice1["eta"], eta_slice1["loglik"], 
            yerr=eta_slice1["loglik_se"], fmt='o-', 
            capsize=3, label=r'$\beta=15$')
ax.errorbar(eta_slice2["eta"], eta_slice2["loglik"], 
            yerr=eta_slice2["loglik_se"], fmt='s-', 
            capsize=3, label=r'$\beta=25$')
ax.errorbar(eta_slice3["eta"], eta_slice3["loglik"], 
            yerr=eta_slice3["loglik_se"], fmt='^-', 
            capsize=3, label=r'$\beta=10$')

ax.axvline(theta["eta"], color='gray', linestyle='--', alpha=0.5)
ax.set(xlabel=r'$\eta$ (initial susceptible fraction)', 
       ylabel='Log-likelihood',
       title=r'Likelihood slices in $\eta$ direction')
ax.legend()
ax.grid(alpha=0.3)
plt.tight_layout()
plt.show()
```

## Interpretation

The slices reveal:

1. The optimal $\eta$ depends on the value of $\beta$
2. There is a ridge-like relationship between $\eta$ and $\beta$
3. This suggests these parameters may be partially **non-identifiable** or have a **functional relationship**


# Exercise 3.5: 2D Likelihood Surface

## Problem Statement

Compute a slice of the likelihood in the $\beta$-$\eta$ plane.

## Solution {.allowframebreaks}

```{python}
#| echo: true
def compute_2d_surface(param1_name, param1_vals, param2_name, param2_vals,
                       base_theta, J=2000, n_reps=2):
    """Compute 2D log-likelihood surface."""
    results = []
    
    for v1 in param1_vals:
        for v2 in param2_vals:
            theta_test = base_theta.copy()
            theta_test[param1_name] = v1
            theta_test[param2_name] = v2
            
            pomp_test = pp.Pomp(
                rinit=rinit, rproc=rproc, dmeas=dmeas, rmeas=rmeas,
                ys=ys, theta=theta_test, statenames=statenames,
                t0=0.0, nstep=7, accumvars=(3,), ydim=1, covars=None
            )
            
            key = jax.random.key(int(v1 * 1000) + int(v2 * 10000))
            pomp_test.pfilter(key=key, J=J, reps=n_reps)
            
            pf_result = pomp_test.results_history.last()
            logliks_arr = pf_result.logLiks.values[0, :]
            
            results.append({
                param1_name: v1,
                param2_name: v2,
                'loglik': pp.logmeanexp(logliks_arr)
            })
    
    return pd.DataFrame(results)
```

\framebreak

```{python}
#| echo: true
# Compute 2D surface
beta_grid = np.linspace(8, 35, 15)
eta_grid = np.linspace(0.02, 0.10, 15)

print("Computing 2D likelihood surface...")
surface_df = compute_2d_surface(
    "Beta", beta_grid, "eta", eta_grid, theta, J=2000, n_reps=2
)
print("Done!")
```

\framebreak

```{python}
#| echo: true
# Create contour plot
pivot = surface_df.pivot(index='eta', columns='Beta', values='loglik')

fig, ax = plt.subplots(figsize=(6, 5))
X, Y = np.meshgrid(pivot.columns, pivot.index)
Z = pivot.values

# Mask values too far from maximum
max_ll = np.nanmax(Z)
Z_masked = np.where(Z > max_ll - 30, Z, np.nan)

# Contour plot
contour = ax.contourf(X, Y, Z_masked, levels=20, cmap='viridis')
plt.colorbar(contour, ax=ax, label='Log-likelihood')

# Add contour lines
contour_lines = ax.contour(X, Y, Z_masked, levels=10, colors='white', 
                           linewidths=0.5, alpha=0.5)

# Mark current parameters
ax.plot(theta["Beta"], theta["eta"], 'r*', markersize=15, 
        label='Current params')

ax.set(xlabel=r'$\beta$ (transmission rate)', 
       ylabel=r'$\eta$ (initial susceptible fraction)',
       title=r'Likelihood surface in $\beta$-$\eta$ plane')
ax.legend()
plt.tight_layout()
plt.show()
```

\framebreak

```{python}
#| echo: true
# Find approximate MLE location
max_idx = surface_df['loglik'].idxmax()
mle_beta = surface_df.loc[max_idx, 'Beta']
mle_eta = surface_df.loc[max_idx, 'eta']
mle_ll = surface_df.loc[max_idx, 'loglik']

print(f"Approximate MLE from grid search:")
print(f"  Beta = {mle_beta:.2f}")
print(f"  eta = {mle_eta:.4f}")
print(f"  Log-likelihood = {mle_ll:.2f}")
```

## Interpretation {.allowframebreaks}

The 2D likelihood surface reveals several important features:

1. **Wedge-shaped relationship**: The likelihood forms a ridge running diagonally through the parameter space, indicating a trade-off between $\beta$ and $\eta$.

2. **Parameter correlation**: Higher $\beta$ (transmission rate) tends to be associated with lower $\eta$ (initial susceptibles) at the maximum likelihood.

3. **Non-identifiability**: The elongated ridge suggests that $\beta$ and $\eta$ are not fully identifiable from these data alone — many combinations give similar likelihoods.

\framebreak

4. **Monte Carlo noise**: Despite the noise in individual likelihood evaluations, the major structure of the surface is clearly visible.

5. **Optimization challenges**: The wedge shape means that gradient-based optimization may converge slowly along the ridge direction.

These features are **typical** of epidemiological models and motivate:
- Using profile likelihoods for inference
- Fixing some parameters at known values
- Employing specialized optimization algorithms (like iterated filtering)


# Summary

## Key Takeaways

1. **Slices vs Profiles**: Slices fix other parameters; profiles optimize them. Profiles are needed for valid statistical inference.

2. **Computational scaling**: Particle filter time scales linearly with number of particles $J$.

3. **Bias in log-likelihood**: Particle filter estimates of log-likelihood are negatively biased; use `logmeanexp` and large $J$ to reduce bias.

4. **2D surfaces**: Reveal relationships between parameters and potential identifiability issues.


# References
