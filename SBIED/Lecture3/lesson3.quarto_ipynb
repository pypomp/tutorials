{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: >\n",
        "  Lesson¬†3: Likelihood-based inference for POMP models\n",
        "author:\n",
        "  - Aaron¬†A.¬†King\n",
        "  - Edward¬†L.¬†Ionides\n",
        "  - Translated in pypomp by Kunyang¬†He\n",
        "\n",
        "shortauthor: \"King¬†&¬†Ionides‚ÄØet‚ÄØal.\"   \n",
        "shorttitle: \"Lesson¬†3\"  \n",
        "date: \"July¬†23,¬†2025\"\n",
        "\n",
        "bibliography: sbied.bib                \n",
        "\n",
        "format:\n",
        "  beamer:\n",
        "    code-block-font-size: \\tiny                    \n",
        "    theme: AnnArbor\n",
        "    colortheme: default\n",
        "    fontsize: 11pt\n",
        "    cite-method: natbib\n",
        "    biblio-style: apalike\n",
        "    toc: true  \n",
        "    slide-level: 3\n",
        "    highlight-style: tango  \n",
        "  \n",
        "  pdf:\n",
        "    documentclass: article          \n",
        "    fontsize: 11pt\n",
        "    cite-method: natbib           \n",
        "    biblio-style: apalike\n",
        "    toc: true\n",
        "    geometry: margin=1in\n",
        "    \n",
        "jupyter: python3 \n",
        "\n",
        "\n",
        "\n",
        "header-includes: |\n",
        "  \\providecommand{\\AtBeginSection}[1]{}\n",
        "  \\providecommand{\\AtBeginSubsection}[1]{}\n",
        "  \\providecommand{\\framebreak}{}\n",
        "  \\AtBeginSection{}\n",
        "  \\AtBeginSubsection{}\n",
        "  \\usepackage{amsmath,amssymb,amsfonts}\n",
        "  \\usepackage{graphicx}\n",
        "  \\usepackage{hyperref}\n",
        "  \\usepackage{xcolor}\n",
        "  \\usepackage{tikz}\n",
        "  \\usetikzlibrary{positioning,calc,arrows.meta,shapes.geometric}\n",
        "  \\usetikzlibrary{positioning,calc,arrows.meta}\n",
        "  \\usepackage{fvextra}  \n",
        "  \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{\n",
        "  commandchars=\\\\\\{\\},\n",
        "  fontsize=\\scriptsize\n",
        "  }\n",
        "\n",
        "  \\newcommand{\\myemph}[1]{\\emph{#1}}\n",
        "  \\newcommand{\\deriv}[2]{\\frac{d #1}{d #2}}\n",
        "  \\newcommand{\\pkg}[1]{\\texttt{#1}}\n",
        "  \\newcommand{\\code}[1]{\\texttt{#1}}\n",
        "  \\newcommand{\\Rlanguage}{\\textsf{R}}\n",
        "  \\newcommand{\\Rzero}{\\mathcal{R}_{0}}\n",
        "  \\newcommand{\\pr}{\\mathbb{P}}\n",
        "  \\newcommand{\\equals}{=}\n",
        "  \\newcommand{\\dist}[2]{\\operatorname{#1}\\!\\bigl(#2\\bigr)}\n",
        "  \\newcommand{\\Sexpr}[1]{\\textcolor{gray}{[\\detokenize{#1}]}}\n",
        "  \\newcommand{\\myexercise}{\\paragraph{Exercise}}\n",
        "  \\newcommand{\\lik}{\\mathcal{L}}   % ùìõ(Œ∏)\n",
        "  \\newcommand{\\loglik}{\\ell}       % ‚Ñì(Œ∏)\n",
        "  \\newcommand{\\prob}[1]{\\mathbb{P}\\!\\left(#1\\right)}\n",
        "  \\newcommand{\\E}{\\mathbb{E}}\n",
        "  \\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
        "  \\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
        "  \\newcommand{\\profileloglik}{\\ell_{\\mathrm p}}\n",
        "  \\definecolor{grey}{gray}{0.5}\n",
        "  \\newcommand{\\R}{\\mathbb{R}}\n",
        "  \\newcommand{\\C}{\\mathbb{C}}\n",
        "  \\newcommand{\\N}{\\mathbb{N}}\n",
        "\n",
        "---\n",
        "\n",
        "# Introduction\n",
        "\n",
        "### Objectives\n",
        "\n",
        "Students completing this lesson will:\n",
        "\n",
        "1. Gain an understanding of the nature of the problem of likelihood computation for POMP models.\n",
        "2. Be able to explain the simplest particle filter algorithm.\n",
        "3. Gain experience in the visualization and exploration of likelihood surfaces.\n",
        "4. Be able to explain the tools of likelihood‚Äëbased statistical inference that become available given numerical accessibility of the likelihood function.\n",
        "\n",
        "### Overview {.allowframebreaks}\n",
        "\n",
        "The following schematic diagram represents conceptual links between different components of the methodological approach we're developing for statistical inference on epidemiological dynamics.\n",
        "\n",
        "![flowchart](flowchart.jpg){height=4cm}\n",
        "\n",
        "\\framebreak\n",
        "\n",
        "- In this lesson, we're going to discuss the orange compartments.\n",
        "- The Monte Carlo technique called the ``particle filter'' is central for connecting the higher-level ideas of POMP models and likelihood-based inference to the lower-level tasks involved in carrying out data analysis.\n",
        "- We employ a standard toolkit for likelihood based inference: Maximum likelihood estimation, profile likelihood confidence intervals, likelihood ratio tests for model selection, and other likelihood-based model comparison tools such as AIC.\n",
        "- We seek to better understand these tools, and to figure out how to implement and interpret them in the specific context of POMP models.\n",
        "\n",
        "\n",
        "# The likelihood function\n",
        "\n",
        "## General considerations\n",
        "\n",
        "### The likelihood function\n",
        "- The basis for modern frequentist, Bayesian, and information‚Äëtheoretic inference.  \n",
        "- Method of maximum likelihood introduced by \\citet{Fisher1922}.  \n",
        "- The likelihood function itself is a representation of the what the data have to say about the parameters.  \n",
        "- A good general reference on likelihood is by \\citet{Pawitan2001}.  \n",
        "\n",
        "### Definition of the likelihood function\n",
        "- Data are a sequence of¬†$N$ observations, denoted¬†$y_{1:N}^*$.  \n",
        "- A statistical model is a density function¬†$f_{Y_{1:N}}(y_{1:N};\\theta)$ which defines a probability distribution for each value of a parameter vector¬†$\\theta$.  \n",
        "- To perform statistical inference, we must decide, among other things, for which (if any) values of¬†$\\theta$ it is reasonable to model¬†$y^*_{1:N}$ as a random draw from¬†$f_{Y_{1:N}}(y_{1:N};\\theta)$.  \n",
        "- The likelihood function is  \n",
        "  $$\n",
        "  \\lik(\\theta) = f_{Y_{1:N}}(y^*_{1:N};\\theta),\n",
        "  $$ \n",
        "\n",
        "\n",
        "  the density function evaluated at the data.  \n",
        "- It is often convenient to work with the log‚Äëlikelihood function,  \n",
        "  $$\n",
        "  \\loglik(\\theta)= \\log \\lik(\\theta) = \\log f_{Y_{1:N}}(y^*_{1:N};\\theta).\n",
        "  $$  \n",
        "\n",
        "### Modeling using discrete and continuous distributions\n",
        "- Recall that the probability distribution¬†$f_{Y_{1:N}}(y_{1:N};\\theta)$ defines a random variable¬†$Y_{1:N}$ for which probabilities can be computed as integrals of¬†$f_{Y_{1:N}}(y_{1:N};\\theta)$.  \n",
        "- Specifically, for any event¬†$E$ describing a set of possible outcomes of¬†$Y_{1:N}$,  \n",
        "  $$\n",
        "  \\prob{Y_{1:N} \\in E} = \\int_E f_{Y_{1:N}}(y_{1:N};\\theta)\\, dy_{1:N}.\n",
        "  $$  \n",
        "\n",
        "\n",
        "\n",
        "- If the model corresponds to a discrete distribution, then the integral is replaced by a sum and the probability density function is called a *probability mass function*.  \n",
        "- The definition of the likelihood function remains unchanged.  \n",
        "  We will use the notation of continuous random variables, but all the methods apply also to discrete models.  \n",
        "\n",
        "### A simulator is implicitly a statistical model\n",
        "- For simple statistical models, we may describe the model by explicitly writing the density function¬†$f_{Y_{1:N}}(y_{1:N};\\theta)$. One may then ask how to simulate a random variable¬†$Y_{1:N}\\sim f_{Y_{1:N}}(y_{1:N};\\theta)$.  \n",
        "- For many dynamic models it is much more convenient to define the model via a procedure to simulate the random variable¬†$Y_{1:N}$. This *implicitly* defines the corresponding density¬†$f_{Y_{1:N}}(y_{1:N};\\theta)$.  \n",
        "- For a complicated simulation procedure, it may be difficult or impossible to write down or even compute¬†$f_{Y_{1:N}}(y_{1:N};\\theta)$ exactly.  \n",
        "- It is important to bear in mind that the likelihood function exists even when we don't know what it is!  \n",
        "  We can still talk about the likelihood function, and develop numerical methods that take advantage of its statistical properties.  \n",
        "\n",
        "### The likelihood for a POMP model {.allowframebreaks}\n",
        "\n",
        "- Recall the following schematic diagram, showing dependence among variables in a POMP model.  \n",
        "- Measurements, $Y_n$, at time¬†$t_n$ depend on the latent process, $X_n$, at that time.  \n",
        "- The Markov property asserts that latent process variables depend on their value at the previous timestep.  \n",
        "- To be more precise, the distribution of the state¬†$X_{n+1}$, conditional on¬†$X_{n}$, is independent of the values of¬†$X_{k}$, $k<n$, and¬†$Y_{k}$, $k\\le n$.  \n",
        "- Moreover, the distribution of the measurement¬†$Y_{n}$, conditional on¬†$X_{n}$, is independent of all other variables.  \n",
        "\n",
        "\n",
        "\n",
        "\\begin{figure}[htbp]     \n",
        "  \\centering             \n",
        "  \\begin{tikzpicture}[\n",
        "      scale=0.7,\n",
        "      every node/.style={transform shape},\n",
        "      node distance=1.5cm, auto, >=Stealth]\n",
        "    \n",
        "      % Styles\n",
        "  \\tikzstyle{state} = [rectangle, draw, minimum size=1cm, font=\\small]\n",
        "  \\tikzstyle{observation} = [rectangle, draw, minimum size=1cm, font=\\small]\n",
        "  \\tikzstyle{label} = [font=\\bfseries\\small, align=center]\n",
        "  \\tikzstyle{model} = [font=\\itshape\\small]\n",
        "\n",
        "  % Nodes\n",
        "  \\node[state] (x0) {$X_0$};\n",
        "  \\node[state, right=of x0] (x1) {$X_1$};\n",
        "  \\node[right=1cm of x1] (dots1) {$\\cdots$};\n",
        "  \\node[state, right=1cm of dots1] (xn1) {$X_{n-1}$};\n",
        "  \\node[state, right=of xn1] (xn) {$X_n$};\n",
        "  \\node[state, right=of xn] (xnp1) {$X_{n+1}$};\n",
        "  \\node[observation, above=of x1] (y1) {$Y_1$};\n",
        "  \\node[observation, above=of xn1] (yn1) {$Y_{n-1}$};\n",
        "  \\node[observation, above=of xn] (yn) {$Y_n$};\n",
        "  \\node[observation, above=of xnp1] (ynp1) {$Y_{n+1}$};\n",
        "\n",
        "  % Labels\n",
        "  \\node[label, left=1cm of x0] {States};\n",
        "  \\node[label, left=2.25cm of y1] {Observations};\n",
        "\n",
        "  % Time Labels\n",
        "  \\node[below=1.5cm of x0] {$t_0$};\n",
        "  \\node[below=1.5cm of x1] {$t_1$};\n",
        "  \\node[below=1.8cm of dots1] {$\\cdots$};\n",
        "  \\node[below=1.5cm of xn1] {$t_{n-1}$};\n",
        "  \\node[below=1.5cm of xn] {$t_n$};\n",
        "  \\node[below=1.5cm of xnp1] {$t_{n+1}$};\n",
        "\n",
        "  % Paths\n",
        "  \\path[->]\n",
        "    (x0) edge (x1)\n",
        "    (x1) edge (dots1)\n",
        "    (dots1) edge (xn1)\n",
        "    (xn1) edge (xn)\n",
        "    (xn) edge (xnp1)\n",
        "    (x1) edge[dashed] (y1)\n",
        "    (xn1) edge[dashed] (yn1)\n",
        "    (xn) edge[dashed] (yn)\n",
        "    (xnp1) edge[dashed] (ynp1);\n",
        "\n",
        "  % Model Labels\n",
        "  \\node[model, below=1.0cm of dots1, align=center] (processmodel) {Process model};\n",
        "  \\node[model, above=3.3cm of dots1, align=center] (measurementmodel) {Measurement model}; \n",
        "\n",
        "\\usetikzlibrary{calc} \n",
        "  % Coordinate for new arrow\n",
        "  \\coordinate (midarrow) at ($(x1)!0.5!(y1)$);\n",
        "  \\coordinate (midbrrow) at ($(xn1)!0.5!(yn1)$);\n",
        "  \\coordinate (midcrrow) at ($(xn)!0.5!(yn)$);\n",
        "  \\coordinate (middrrow) at ($(xnp1)!0.5!(ynp1)$);\n",
        "  \\coordinate (miderrow) at ($(x0)!0.5!(x1)$);\n",
        "  \\coordinate (midfrrow) at ($(x1)!0.5!(xn1)$);\n",
        "  \\coordinate (midgrrow) at ($(xn1)!0.5!(xn)$);\n",
        "  \\coordinate (midhrrow) at ($(xn)!0.5!(xnp1)$);\n",
        "  \n",
        "  % New arrow\n",
        "  \\draw[->, dashed] (midarrow) -- (measurementmodel);\n",
        "  \\draw[->, dashed] (midbrrow) -- (measurementmodel);\n",
        "  \\draw[->, dashed] (midcrrow) -- (measurementmodel);\n",
        "  \\draw[->, dashed] (middrrow) -- (measurementmodel);\n",
        "  \\draw[->, dashed] (miderrow) -- (processmodel);\n",
        "  \\draw[->, dashed] (midfrrow) -- (processmodel);\n",
        "  \\draw[->, dashed] (midgrrow) -- (processmodel);\n",
        "  \\draw[->, dashed] (midhrrow) -- (processmodel);\n",
        "\n",
        "  \n",
        "  % Add horizontal arrow between t_n and process model\n",
        "  \\coordinate (startarrow) at ($(x0) - (0,2)$);\n",
        "  \\coordinate (endarrow) at ($(xnp1) - (0,2)$);\n",
        "  \\draw[->] (startarrow) -- (endarrow) node[midway, below] {};\n",
        "  \\end{tikzpicture}\n",
        "  \\label{fig:pomp-centered}\n",
        "\\end{figure}\n",
        "\n",
        "\\framebreak\n",
        "\n",
        "- The latent process¬†$X(t)$ may be defined at all times, but we are particularly interested in its value at observation times. Therefore, we write  \n",
        "  $$  \n",
        "  X_n = X(t_n). \n",
        "  $$\n",
        "\n",
        "\n",
        "- We write collections of random variables using the notation¬†$X_{0:N} = (X_0,\\dots,X_N)$.  \n",
        "- The one‚Äëstep transition density ‚Ä¶ specify the entire joint density via  \n",
        "    \\begin{equation*}\n",
        "      \\begin{split}\n",
        "        &f_{X_{0:N},Y_{1:N}}(x_{0:N},y_{1:N};\\theta)\\\\\n",
        "        & \\qquad = f_{X_0}(x_0;\\theta)\\,\\prod_{n=1}^N\\!f_{X_n | X_{n-1}}(x_n|x_{n-1};\\theta)\\,f_{Y_n|X_n}(y_n|x_n;\\theta).\n",
        "      \\end{split}\n",
        "    \\end{equation*}\n",
        "\n",
        "\n",
        "- The marginal density for the sequence of measurements¬†$Y_{1:N}$, evaluated at the data¬†$y_{1:N}^*$, is  \n",
        "\n",
        "  $$\n",
        "    \\lik(\\theta)\n",
        "      = f_{Y_{1:N}}(y_{1:N}^*;\\theta)\n",
        "      = \\int\n",
        "          f_{X_{0:N},Y_{1:N}}(x_{0:N},y_{1:N}^*;\\theta)\\,\n",
        "          dx_{0:N}.\n",
        "  $$\n",
        "\n",
        "### Special case: deterministic latent process\n",
        "- When the latent process is non‚Äërandom, the log‚Äëlikelihood for a POMP model closely resembles a nonlinear regression model.  \n",
        "\n",
        "- In this case, we can write $X_{n}=x_n(\\theta)$, and the log‚Äëlikelihood is  \n",
        "     \\begin{equation*}\n",
        "      \\loglik(\\theta) = \\sum_{n=1}^N \\log f_{Y_n|X_n}\\big(y_n^*| x_n(\\theta); \\theta\\big).\n",
        "    \\end{equation*}\n",
        "\n",
        "- If we have a Gaussian measurement model, where $Y_n$ given $X_n=x_n(\\theta)$ is conditionally normal with mean $\\hat{y}_n\\bigl(x_n(\\theta)\\bigr)$ and constant variance $\\sigma^2$, then the log‚Äëlikelihood contains a sum of squares which is exactly the criterion that nonlinear least‚Äësquares regression seeks to minimize.  \n",
        "- More details on deterministic latent process models are given as a supplement.  \n",
        "\n",
        "\n",
        "### General case: stochastic unobserved state process\n",
        "- For a POMP model, the likelihood takes the form of an integral:  \n",
        "\n",
        "    \\begin{equation}\\label{eq:L1}\n",
        "      \\begin{aligned}\n",
        "        \\lik(\\theta) &= f_{Y_{1:N}}({y^*_{1:N}};\\theta)\\\\\n",
        "        = &\\int f_{X_0}(x_0;\\theta)\\prod_{n=1}^{N}\\!f_{Y_n|X_n}({y^*_n}| x_n; \\theta)\\, f_{X_n|X_{n-1}}(x_n|x_{n-1};\\theta)\\, dx_{0:N}.\n",
        "      \\end{aligned}\n",
        "    \\end{equation}\n",
        "\n",
        "\n",
        "\n",
        "- This integral is high‚Äëdimensional and, except for the simplest cases, cannot be reduced analytically.  \n",
        "\n",
        "# Computing the likelihood\n",
        "\n",
        "## Monte Carlo algorithms\n",
        "\n",
        "### Monte Carlo likelihood by direct simulation {.allowframebreaks}\n",
        "\n",
        "- We work toward introducing the particle filter by first proposing a simpler method that usually doesn't work on anything but very short time series.  \n",
        "- Although **this section is a demonstration of what not to do**, it serves as an introduction to the general approach of Monte¬†Carlo integration.  \n",
        "- First, let's rewrite the likelihood integral using an equivalent factorization. As an exercise, you could check how the equivalence of Eqns.~\\ref{eq:L1} and¬†\\ref{eq:L2} follows algebraically from the Markov property and the definition of conditional density.  \n",
        "\n",
        "\\begin{equation}\\label{eq:L2}\n",
        "  \\begin{aligned}\n",
        "    \\lik(\\theta) &= f_{Y_{1:N}}({y^*_{1:N}};\\theta)\\\\\n",
        "    &= \\int\\!\\left\\{\\prod_{n=1}^{N}\\!f_{Y_n|X_n}({y^*_n}| x_n; \\theta)\\right\\}\\,f_{X_{0:N}}(x_{0:N};\\theta)\\, dx_{0:N}.\n",
        "  \\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "- Notice, using the representation in Eqn.~\\ref{eq:L2}, that the likelihood can be written as an expectation,  \n",
        "  \\begin{equation*}\n",
        "    \\lik(\\theta) = \\E \\left[ \\prod_{n=1}^{N}\\!f_{Y_n|X_n}({y^*_n}| X_n; \\theta) \\right],\n",
        "  \\end{equation*}\n",
        "  where the expectation is taken with $X_{0:N}\\sim f_{X_{0:N}}(x_{0:N};\\theta)$.  \n",
        "- Now, using a law of large numbers, we can approximate an expectation by the average of a Monte¬†Carlo sample. Thus,  \n",
        "  \\begin{equation*}\n",
        "    \\lik(\\theta) \\approx \\frac{1}{J} \\sum_{j=1}^{J}\\prod_{n=1}^{N}\\!f_{Y_n|X_n}({y^*_n}| X^j_n; \\theta),\n",
        "  \\end{equation*}\n",
        "  where $\\{X^j_{0:N}, j=1,\\dots,J\\}$ is a Monte¬†Carlo sample of size $J$ drawn from $f_{X_{0:N}}(x_{0:N};\\theta)$.  \n",
        "- We see that, if we generate trajectories by simulation, all we have to do to get a Monte¬†Carlo estimate of the likelihood is evaluate the measurement density of the data at each trajectory and average.  \n",
        "- We get the **plug‚Äëand‚Äëplay** property that our algorithm depends on `rprocess` but does not require `dprocess`.  \n",
        "- However, this naive approach scales poorly with dimension. It requires a Monte¬†Carlo effort that scales exponentially with the length of the time series, and so is infeasible on anything but a short data set.  \n",
        "- One way to see this is to notice that, once a simulated trajectory diverges from the data, it will seldom come back. Simulations that lose track of the data will make a negligible contribution to the likelihood estimate. When simulating a long time series, almost all the simulated trajectories will eventually lose track of the data.  \n",
        "- We can see this happening in practice for the measles outbreak data.\n",
        "\n",
        "\n",
        "## Sequential Monte Carlo\n",
        "\n",
        "### Sequential Monte Carlo: The particle filter {.allowframebreaks}\n",
        "\n",
        "- Fortunately, we can compute the likelihood for a POMP model by a much more efficient algorithm than direct Monte¬†Carlo integration.\n",
        "- We proceed by factorizing the likelihood in a different way:\n",
        "\n",
        "\\begin{equation*}\n",
        "  \\begin{aligned}\n",
        "    \\lik(\\theta)&=f_{Y_{1:N}}(y^*_{1:N}; \\theta)\n",
        "      =\\prod_{n=1}^N\\,f_{Y_n|Y_{1:n-1}}(y^*_n|y^*_{1:n-1};\\theta)\\\\\n",
        "      &=\\prod_{n=1}^N\\,\\int f_{Y_n|X_n}(y^*_n|x_n;\\theta)\\,\n",
        "        f_{X_n|Y_{1:n-1}}(x_n|y^*_{1:n-1};\\theta)\\,dx_{n},\n",
        "  \\end{aligned}\n",
        "\\end{equation*}\n",
        "\n",
        "with the understanding that $f_{X_1|Y_{1:0}}=f_{X_1}$.\n",
        "\n",
        "- The Markov property leads to the **prediction formula:**\n",
        "\n",
        "\\begin{equation*}\n",
        "  \\begin{aligned}\n",
        "    &f_{X_n|Y_{1:n-1}}(x_n|y^*_{1:n-1}; \\theta) \\\\\n",
        "    &\\quad = \\int f_{X_n|X_{n-1}}(x_n|x_{n-1};\\theta)\\,\n",
        "      f_{X_{n-1}|Y_{1:n-1}}(x_{n-1}|y^*_{1:n-1}; \\theta)\\,dx_{n-1}.\n",
        "  \\end{aligned}\n",
        "\\end{equation*}\n",
        "\n",
        "- Bayes' theorem gives the **filtering formula:**\n",
        "\n",
        "\\begin{equation*}\n",
        "  \\begin{aligned}\n",
        "    &f_{X_n|Y_{1:n}}(x_n|y^*_{1:n}; \\theta)\\\\\n",
        "    &\\quad = f_{X_n|Y_n,Y_{1:n-1}}(x_n|y^*_n,y^*_{1:n-1}; \\theta) \\\\\n",
        "    &\\quad =\\frac{f_{Y_n|X_n}(y^*_{n}|x_{n};\\theta)\\,\n",
        "      f_{X_n|Y_{1:n-1}}(x_{n}|y^*_{1:n-1};\\theta)}\n",
        "      {\\int f_{Y_n|X_n}(y^*_{n}|u_{n};\\theta)\\,\n",
        "      f_{X_n|Y_{1:n-1}}(u_{n}|y^*_{1:n-1};\\theta)\\,du_n}.\n",
        "  \\end{aligned}\n",
        "\\end{equation*}\n",
        "\n",
        "\\framebreak\n",
        "\n",
        "- This suggests that we keep track of two key distributions at each time $t_n$,\n",
        "- The **prediction distribution** is $f_{X_n | Y_{1:n-1}}(x_n| y^*_{1:n-1})$.\n",
        "- The **filtering distribution** is $f_{X_{n} | Y_{1:n}}(x_n| y^*_{1:n})$.\n",
        "- The prediction and filtering formulas give us a two-step recursion:\n",
        "  - The prediction formula gives the prediction distribution at time $t_n$ using the filtering distribution at time $t_{n-1}$.\n",
        "  - The filtering formula gives the filtering distribution at time $t_n$ using the prediction distribution at time $t_n$.\n",
        "- The **particle filter** use Monte¬†Carlo techniques to sequentially estimate the integrals in the prediction and filtering recursions. Hence, the alternative name of **sequential Monte¬†Carlo (SMC)**.\n",
        "\n",
        "\\framebreak\n",
        "\n",
        "A basic particle filter is described as follows:\n",
        "\n",
        "\\begin{enumerate}[(1)]\n",
        "\\item Suppose $X_{n-1,j}^{F}$, $j=1,\\dots,J$ is a set of $J$ points drawn from the filtering distribution at time $t_{n-1}$.\n",
        "\\item We obtain a sample $X_{n,j}^{P}$ of points drawn from the prediction distribution at time $t_n$ by simply simulating the process model:\n",
        "  \\begin{equation*}\n",
        "    X_{n,j}^{P} \\sim \\mathrm{process}(X_{n-1,j}^{F},\\theta), \\qquad j=1,\\dots,J.\n",
        "  \\end{equation*}\n",
        "\\item Having obtained $x_{n,j}^{P}$, we obtain a sample of points from the filtering distribution at time $t_n$ by \\emph{resampling} from $\\big\\{X_{n,j}^{P},j\\in 1:J\\big\\}$ with weights\n",
        "  \\begin{equation*}\n",
        "    w_{n,j}=f_{Y_n|X_n}(y^*_{n}|X^P_{n,j};\\theta).\n",
        "  \\end{equation*}\n",
        "\\item The Monte¬†Carlo principle tells us that the conditional likelihood\n",
        "  \\begin{equation*}\n",
        "    \\begin{aligned}\n",
        "      \\lik_n(\\theta) &= f_{Y_n|Y_{1:n-1}}(y^*_n|y^*_{1:n-1};\\theta)\\\\\n",
        "      &= \\int f_{Y_n|X_n}(y^*_{n}|x_{n};\\theta)\\,\n",
        "        f_{X_n|Y_{1:n-1}}(x_{n}|y^*_{1:n-1};\\theta)\\,dx_n\n",
        "    \\end{aligned}\n",
        "  \\end{equation*}\n",
        "  is approximated by\n",
        "  $$\n",
        "    \\hat{\\lik}_n(\\theta)\\approx\\frac{1}{J}\\sum_j\n",
        "      f_{Y_n|X_n}(y^*_{n}|X_{n,j}^{P};\\theta)\n",
        "  $$\n",
        "  since $X_{n,j}^{P}$ is approximately a draw from\n",
        "  $f_{X_n|Y_{1:n-1}}(x_{n}|y^*_{1:n-1};\\theta)$.\n",
        "\\item We can iterate this procedure through the data, one step at a time, alternately simulating and resampling, until we reach $n=N$.\n",
        "\\item The full log-likelihood then has approximation\n",
        "  \\begin{equation*}\n",
        "    \\loglik(\\theta)=\\log\\lik(\\theta)=\\sum_n\\log\\lik_n(\\theta)\n",
        "      \\approx \\sum_n\\log\\hat{\\lik}_n(\\theta).\n",
        "  \\end{equation*}\n",
        "\\end{enumerate}\n",
        "\n",
        "\\framebreak\n",
        "\n",
        "- References on the particle filter include \\citet{Kitagawa1987}, \\citet{Arulampalam2002}, \\citet{Doucet2001}, \\citet{King2016}.\n",
        "- It can be shown that the particle filter provides an unbiased estimate of the likelihood. This implies a consistent but biased estimate of the log-likelihood.\n",
        "\n",
        "### A block diagram representation of a particle filter\n",
        "\n",
        "\\vspace{-2mm}\n",
        "\\begin{center}\n",
        "\\begin{tikzpicture}[\n",
        "  square/.style={rectangle, draw=black, minimum width=4.2cm, minimum height=0.7cm, rounded corners=.1cm,font=\\ttfamily},\n",
        "  >/.style={shorten >=0.4mm}, % redefine arrow to stop short of node\n",
        "  >/.tip={Stealth[length=2.5mm,width=1.5mm]} % redefine arrow style\n",
        "]\n",
        "\\tikzset{>={}}; % this is needed to implement the arrow redefinition\n",
        "  \\node (initialize)   at (0,6)  [square] {initialize: rinit};\n",
        "  \\node (predict)  at (0,4.5)  [square] {predict: rprocess};\n",
        "  \\node (weight)  at (0,3)  [square] {weight: dmeasure};\n",
        "  \\node (filter)  at (0,1.5)  [square] {filter: resample};\n",
        "  \\node (N) at (5.5,3)  [draw,diamond,aspect=2.5] {\\texttt{N observations}};\n",
        "  \\node (Np) at (0,7.5)  [draw,diamond,aspect=2.5,color=red] {\\texttt{Np particles}};\n",
        "  \\draw[->] (initialize) -- (predict);\n",
        "  \\draw[->] (predict) -- (weight);\n",
        "  \\draw[->] (weight) -- (filter);\n",
        "  \\draw[->] (filter.east) -| (N);\n",
        "  \\draw[->] (N.north) |- (predict.east);\n",
        "  \\draw[color=red, rounded corners, thick, dashed] (Np.east) -| (2.7,0.7) -- (-2.7,0.7) |- (Np.west);\n",
        "\\end{tikzpicture}\n",
        "\\end{center}\n",
        "\n",
        "### Particle filtering in \\pkg{pypomp} {.allowframebreaks}\n",
        "\n",
        "Here, we'll get some practical experience with the particle filter, and the likelihood function, in the context of our measles-outbreak case study. Here, we repeat the construction of the SIR model, using the parameters chosen by looking at simulations. We can run a few particle filters to get an estimate of the Monte Carlo variability:\n"
      ],
      "id": "0a01c81c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "import jax.numpy as jnp\n",
        "import jax.random\n",
        "import jax.scipy as jsp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pypomp as pp\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from typing import Dict, List\n",
        "\n",
        "\n",
        "\n",
        "meas = (pd.read_csv(\n",
        "    \"https://kingaa.github.io/sbied/stochsim/Measles_Consett_1948.csv\")\n",
        "        .loc[:, [\"week\", \"cases\"]]\n",
        "        .rename(columns={\"week\": \"time\", \"cases\": \"reports\"})\n",
        "        .set_index(\"time\")               \n",
        "        .astype(float)                   \n",
        ")\n",
        "\n",
        "meas = meas.loc[meas.index <= 42]\n",
        "\n",
        "ys = meas.copy()                         \n",
        "ys.columns = pd.Index([\"reports\"])       \n",
        "\n",
        "def unpack_params(theta_vec):\n",
        "    Beta   = theta_vec[0]   # 15\n",
        "    mu_IR  = theta_vec[1]   # 0.5\n",
        "    N      = theta_vec[2]   # 38000\n",
        "    eta    = theta_vec[3]   # 0.06\n",
        "    rho    = theta_vec[4]   # 0.5\n",
        "    k      = theta_vec[5]   # 10\n",
        "    return Beta, mu_IR, N, eta, rho, k\n",
        "\n",
        "\n",
        "def pack_params(Beta, mu_IR, N, eta, rho, k):\n",
        "    return jnp.array([Beta, mu_IR, N, eta, rho, k])\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "@partial(pp.RInit, t0=0.0)\n",
        "def rinit(theta_, key, covars=None, t0=None):\n",
        "    Beta, mu_IR, N, eta, rho, k = unpack_params(theta_)\n",
        "    S0 = jnp.round(N * eta)\n",
        "    I0 = 1.0\n",
        "    R0 = jnp.round(N * (1 - eta))\n",
        "    H0 = 0.0                             \n",
        "    return jnp.array([S0, I0, R0, H0])\n",
        "\n",
        "\n",
        "@partial(pp.RProc, step_type=\"euler\", dt=1/7, accumvars=(3,))\n",
        "def rproc(X_, theta_, key, covars=None, t=None, dt=None):\n",
        "    Beta, mu_IR, N, eta, rho, k = unpack_params(theta_)\n",
        "    S, I, R, H = X_\n",
        "\n",
        "    p_SI = 1.0 - jnp.exp(-Beta * I / N * dt)\n",
        "    p_IR = 1.0 - jnp.exp(-mu_IR * dt)\n",
        "\n",
        "    key_SI, key_IR = jax.random.split(key)\n",
        "    dN_SI = jax.random.binomial(key_SI, n=jnp.round(S).astype(jnp.int32), p=p_SI)\n",
        "    dN_IR = jax.random.binomial(key_IR, n=jnp.round(I).astype(jnp.int32), p=p_IR)\n",
        "\n",
        "    S_new = S - dN_SI\n",
        "    I_new = I + dN_SI - dN_IR\n",
        "    R_new = R + dN_IR\n",
        "    H_new = H + dN_IR           \n",
        "\n",
        "    return jnp.array([S_new, I_new, R_new, H_new])\n",
        "\n",
        "\n",
        "def nbinom_logpmf(x, k, mu):\n",
        "    \"\"\"Log PMF of NB(k, mu) that is robust when mu == 0.\"\"\"\n",
        "    x = jnp.asarray(x)\n",
        "    k = jnp.asarray(k)\n",
        "    mu = jnp.asarray(mu)\n",
        "\n",
        "    # handle mu == 0 separately\n",
        "    logp_zero = jnp.where(x == 0, 0.0, -jnp.inf)\n",
        "\n",
        "    safe_mu = jnp.where(mu == 0.0, 1.0, mu)        # dummy value, ignored\n",
        "    core = (jax.scipy.special.gammaln(k + x) - jax.scipy.special.gammaln(k)\n",
        "            - jax.scipy.special.gammaln(x + 1)\n",
        "            + k * jnp.log(k / (k + safe_mu))\n",
        "            + x * jnp.log(safe_mu / (k + safe_mu)))\n",
        "\n",
        "    return jnp.where(mu == 0.0, logp_zero, core)\n",
        "\n",
        "\n",
        "@pp.DMeas\n",
        "def dmeas(Y_, X_, theta_, covars=None, t=None):\n",
        "    Beta, mu_IR, N, eta, rho, k = unpack_params(theta_)\n",
        "    H = X_[3]\n",
        "    mu = rho * H\n",
        "    return nbinom_logpmf(Y_[0], k, mu)\n",
        "\n",
        "\n",
        "\n",
        "def rnbinom(key, k, mu):\n",
        "    key_g, key_p = jax.random.split(key)\n",
        "    lam = jax.random.gamma(key_g, k) * (mu / k)\n",
        "    return jax.random.poisson(key_p, lam)\n",
        "\n",
        "@partial(pp.RMeas, ydim=1)\n",
        "def rmeas(X_, theta_, key, covars=None, t=None):\n",
        "    Beta, mu_IR, N, eta, rho, k = unpack_params(theta_)\n",
        "    H = X_[3]\n",
        "    mu = rho * H\n",
        "    reports = rnbinom(key, k, mu)\n",
        "    return jnp.array([reports])\n",
        "\n",
        "theta_guess = {\"Beta\": 15, \"mu_IR\": 0.5, \"N\": 38000,\n",
        "               \"eta\": 0.06, \"rho\": 0.5, \"k\": 10.0}\n",
        "\n",
        "param_bounds = {k: (1 * v, 1 * v) for k, v in theta_guess.items()}\n",
        "key = jax.random.key(2)\n",
        "key, subkey = jax.random.split(key)\n",
        "theta_list = pp.Pomp.sample_params(param_bounds, n=1, key=subkey)\n",
        "\n",
        "\n",
        "sir_obj = pp.Pomp(\n",
        "    rinit=rinit,\n",
        "    rproc=rproc,\n",
        "    dmeas=dmeas,\n",
        "    rmeas=rmeas,\n",
        "    ys=ys,\n",
        "    theta=theta_list,\n",
        "    covars=None,\n",
        ")\n",
        "\n",
        "key, sim_key = jax.random.split(key)\n",
        "sim_out = sir_obj.simulate(key=sim_key)    \n",
        "sim_reports = pd.DataFrame(\n",
        "    sim_out[0][\"Y_sims\"].squeeze(),\n",
        "    index=ys.index, columns=[\"reports_sim\"]\n",
        ")\n",
        "print(sim_reports.head())\n",
        "\n",
        "# --- Pull the first replicate (rep index 0) of the first Œ∏-set (index 0)\n",
        "# sim_out[0][\"Y_sims\"] has shape (T, reps, ydim)  ‚Üí  (num_weeks, 1, 1) here\n",
        "sim_array   = sim_out[0][\"Y_sims\"][:, 0, 0]        # drop the last two singleton axes\n",
        "sim_reports = pd.Series(sim_array,\n",
        "                        index=ys.index,\n",
        "                        name=\"reports_sim\")"
      ],
      "id": "b8f11215",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "sir_obj.pfilter(J=1000, reps=10)\n",
        "sir_obj.results()"
      ],
      "id": "49aa3fd9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Likelihood-based inference\n",
        "\n",
        "## Parameter estimates and uncertainty quantification\n",
        "\n",
        "### Review of likelihood-based inference  \n",
        "\n",
        "For now, let us suppose that software exists to evaluate and maximize the likelihood function, up to a tolerable numerical error, for the dynamic models of interest. Our immediate task is to think about how to use that capability.\n",
        "\n",
        "- Likelihood-based inference (meaning statistical tools based on the likelihood function) provides tools for parameter estimation, standard errors, hypothesis tests and diagnosing model misspecification.  \n",
        "- Likelihood-based inference often (but not always) has favorable theoretical properties.  \n",
        "  Here, we are not especially concerned with the underlying theory of likelihood-based inference.  \n",
        "  On any practical problem, we can check the properties of a statistical procedure by simulation experiments.  \n",
        "\n",
        "### The maximum likelihood estimate (MLE)  \n",
        "\n",
        "- A maximum likelihood estimate (MLE) is  \n",
        "  \\begin{equation*}\n",
        "      \\hat\\theta = \\argmax_{\\theta} \\loglik(\\theta),\n",
        "    \\end{equation*}\n",
        "  where $\\argmax_{\\theta} g(\\theta)$ means a value of argument $\\theta$ at which the maximum of the function $g$ is attained, so $g\\left(\\argmax_{\\theta} g(\\theta)\\right) = \\max_\\theta g(\\theta)$.  \n",
        "- If there are many values of $\\theta$ giving the same maximum value of the likelihood, then an MLE still exists but is not unique.  \n",
        "- Note that $\\argmax_{\\theta} \\lik(\\theta)$ and $\\argmax_{\\theta} \\loglik(\\theta)$ are the same.  \n",
        "  Why?  \n",
        "\n",
        "### Standard errors for the MLE {.allowframebreaks}\n",
        "\n",
        "- Parameter estimates are not very useful without some measure of their uncertainty.  \n",
        "- Usually, this means obtaining a confidence interval, or in practice an interval close to a true confidence interval which should formally be called an approximate confidence interval.  \n",
        "  In practice, the word ``approximate'' is often dropped!  \n",
        "\n",
        "\\framebreak  \n",
        "\n",
        "There are three main approaches to estimating the statistical uncertainty in an MLE.  \n",
        "\n",
        "1. The Fisher information.  \n",
        "2. Profile likelihood estimation.  \n",
        "3. A simulation study, also known as a bootstrap.  \n",
        "\n",
        "### Fisher information {.allowframebreaks}\n",
        "\n",
        "- A computationally quick approach when one has access to satisfactory numerical second derivatives of the log‚Äëlikelihood.  \n",
        "- The approximation is satisfactory only when $\\hat\\theta$ is well approximated by a normal distribution.  \n",
        "- Neither of the two requirements above are typically met for POMP models.  \n",
        "- A review of standard errors via Fisher information is provided as a supplement.  \n",
        "\n",
        "### Profile likelihood estimation  \n",
        "\n",
        "This approach is generally preferable to the Fisher information for POMP models.\n",
        "\n",
        "We will explain this method below and put it into practice in the next lesson.  \n",
        "\n",
        "### The bootstrap  \n",
        "\n",
        "- If done carefully and well, this can be the best approach.  \n",
        "- A confidence interval is a claim about reproducibility.  \n",
        "  You claim, so far as your model is correct, that on 95\\% of realizations from the model, a 95\\% confidence interval you have constructed will cover the true value of the parameter.  \n",
        "- A simulation study can check this claim fairly directly, but requires the most effort.  \n",
        "- The simulation study takes time for you to develop and debug, time for you to explain, and time for the reader to understand and check what you have done.  \n",
        "  We usually carry out simulation studies to check our main conclusions only.  \n",
        "- Further discussion of bootstrap methods for POMP models is provided as a supplement.  \n",
        "\n",
        "### Confidence intervals via the profile likelihood {.allowframebreaks}\n",
        "\n",
        "- Let's consider the problem of obtaining a confidence interval for the first component of $\\theta$. We'll write $$\\theta=(\\phi,\\psi).$$  \n",
        "- The **profile log-likelihood function** of $\\phi$ is defined to be  \n",
        " \\begin{equation*}\n",
        "      \\profileloglik{{}}(\\phi) = \\max_{\\psi}\\loglik(\\phi,\\psi).\n",
        "    \\end{equation*} \n",
        "  In general, the profile likelihood of one parameter is constructed by maximizing the likelihood function over all other parameters.  \n",
        "- Note that, $\\max_{\\phi}\\profileloglik{{}}(\\phi) = \\max_{\\theta}\\loglik(\\theta)$ and that maximizing the profile likelihood $\\profileloglik{{}}(\\phi)$ gives the MLE, $\\hat{\\theta}$. Why?  \n",
        "- An approximate 95\\% confidence interval for $\\phi$ is given by  \n",
        "   \\begin{equation*}\n",
        "      \\big\\{\\phi : \\loglik(\\hat\\theta) - \\profileloglik{{}}(\\phi) < 1.92\\big\\}.\n",
        "    \\end{equation*}\n",
        "- This is known as a profile likelihood confidence interval. The cutoff $1.92$ is derived using Wilks' theorem, which we will discuss in more detail when we develop likelihood ratio tests.  \n",
        "- Although the asymptotic justification of Wilks' theorem is the same limit that justifies the Fisher information standard errors, profile likelihood confidence intervals tend to work better than Fisher information confidence intervals when $N$ is not so large---particularly when the log‚Äëlikelihood function is not close to quadratic near its maximum.  \n",
        "\n",
        "# Geometry of the likelihood function\n",
        "\n",
        "### The likelihood surface {.allowframebreaks}\n",
        "\n",
        "- It is extremely useful to visualize the geometric surface defined by the likelihood function.  \n",
        "- If $\\Theta$ is two‚Äëdimensional, then the surface $\\loglik(\\theta)$ has features like a landscape.  \n",
        "- Local maxima of $\\loglik(\\theta)$ are peaks.  \n",
        "- Local minima are valleys.  \n",
        "- Peaks may be separated by a valley or may be joined by a ridge. If you go along the ridge, you may be able to go from one peak to the other without losing much elevation. Narrow ridges can be easy to fall off, and hard to get back on to.  \n",
        "- In higher dimensions, one can still think of peaks and valleys and ridges. However, as the dimension increases it quickly becomes hard to imagine the surface.  \n",
        "\n",
        "### Exploring the likelihood surface: slices  \n",
        "\n",
        "- To get an idea of what the likelihood surface looks like in the neighborhood of a point in parameter space, we can construct some likelihood \\myemph{slices}.  \n",
        "- A likelihood slice is a cross‚Äësection through the likelihood surface.  \n",
        "- We'll make slices for our Consett measles POMP model, in the $\\beta$ and $\\mu_{IR}$ directions.  \n",
        "- Both slices will pass through our current candidate parameter vector, stored in the \\code{pypomp} model object.  \n",
        "\n",
        "### Slicing the measles SIR likelihood\n"
      ],
      "id": "f4299640"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# ---------- 1. slice_design_py ---------------------------------------\n",
        "def slice_design_py(center: dict, **slices):\n",
        "    out = []\n",
        "    for pname, values in slices.items():\n",
        "        for v in values:\n",
        "            th = center.copy()\n",
        "            th[pname] = float(v)\n",
        "            out.append({\n",
        "                \"theta\": th,\n",
        "                \"slice\": pname,\n",
        "                \"Beta\":  th[\"Beta\"],\n",
        "                \"mu_IR\": th[\"mu_IR\"]\n",
        "            })\n",
        "    return out\n",
        "\n",
        "center_theta = {k: float(v) for k, v in theta_guess.items()}\n",
        "\n",
        "beta_vals = np.repeat(np.linspace(5.0, 30.0, 40), 3)\n",
        "mu_vals   = np.repeat(np.linspace(0.2,  2.0, 40), 3)\n",
        "design = slice_design_py(center_theta, Beta=beta_vals, mu_IR=mu_vals)\n",
        "\n",
        "results  = []\n",
        "like_col = 'logLik'  "
      ],
      "id": "7a2de148",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Slicing the measles SIR likelihood II\n"
      ],
      "id": "dc7f9d2d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "for rec in tqdm(design, desc=\"Particle‚Äëfilter grid\"):\n",
        "    sir_obj.pfilter(300, theta=rec[\"theta\"])         \n",
        "    last = sir_obj.results().iloc[-1]\n",
        "    loglik = float(last[like_col])  \n",
        "    results.append({\n",
        "        \"slice\" : rec[\"slice\"],\n",
        "        \"Beta\"  : rec[\"Beta\"],\n",
        "        \"mu_IR\" : rec[\"mu_IR\"],\n",
        "        \"loglik\": loglik\n",
        "    })\n",
        "\n",
        "like_slice = pd.DataFrame(results)"
      ],
      "id": "e0019523",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Slicing the measles SIR likelihood III\n"
      ],
      "id": "da12764d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
        "\n",
        "def plot_slice_ax(df, param, ax):\n",
        "    sub = df[df[\"slice\"] == param]\n",
        "    ax.scatter(sub[param], sub[\"loglik\"], s=10)\n",
        "    ax.set_xlabel(\"parameter value\")\n",
        "    ax.set_ylabel(\"loglik\")\n",
        "    ax.set_title(f\"loglik vs {param}\")\n",
        "\n",
        "plot_slice_ax(like_slice, \"Beta\",  axes[0])\n",
        "plot_slice_ax(like_slice, \"mu_IR\", axes[1])\n",
        "fig.tight_layout()  "
      ],
      "id": "a73e11f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Slicing the measles SIR likelihood IV\n",
        "\n",
        "- Slices offer a very limited perspective on the geometry of the likelihood surface.\n",
        "- When there are only one or two unknown parameters, we can evaluate the likelihood at a grid of points and visualize the surface directly.\n",
        "\n",
        "### Two-dimensional likelihood slice\n"
      ],
      "id": "8aef7fdf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "\n",
        "def logmeanexp(arr: np.ndarray) -> float:\n",
        "    arr = arr[~np.isnan(arr)]\n",
        "    arr = arr[~np.isposinf(arr)]\n",
        "    if len(arr) == 0:\n",
        "        return np.nan\n",
        "    from scipy.special import logsumexp\n",
        "    return float(logsumexp(arr) - np.log(len(arr)))\n",
        "\n",
        "def make_grid() -> List[Dict[str, float]]:\n",
        "    beta_vals = np.repeat(np.linspace(10.0, 30.0, 40), 3)   \n",
        "    mu_vals   = np.repeat(np.linspace(0.4,  1.5, 40), 3)\n",
        "    base = {\"N\": 38_000.0,\"eta\": 0.06, \"rho\": 0.5,\"k\": 10.0}\n",
        "    return [{\"Beta\": float(b), \"mu_IR\": float(m), **base}\n",
        "            for b in beta_vals for m in mu_vals]           \n",
        "\n",
        "J_PF = 500                 \n",
        "_probe = make_grid()[0]\n",
        "\n",
        "sir_obj.pfilter(J_PF, theta=_probe)\n",
        "_last = sir_obj.results().iloc[-1]\n",
        "\n",
        "if   'logLik'     in _last: LIKE_COL, NEGATE = 'logLik', False"
      ],
      "id": "9fa0e0b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Two-dimensional likelihood slice II\n"
      ],
      "id": "d48d04a2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "def pfilter_one(theta: Dict[str, float]) -> float:\n",
        "    sir_obj.pfilter(J_PF, theta=theta)\n",
        "    val = float(sir_obj.results().iloc[-1][LIKE_COL])\n",
        "    if np.isnan(val) or np.isposinf(val):\n",
        "        return np.nan\n",
        "    return -val if NEGATE else val   \n",
        "\n",
        "\n",
        "def run_grid(grid: List[Dict[str, float]]):\n",
        "    return [pfilter_one(t) for t in tqdm(grid, desc=\"pfilter grid\")]\n",
        "\n",
        "def summarize(grid: List[Dict[str, float]], ll: List[float]):\n",
        "    df = pd.DataFrame(grid)\n",
        "    df[\"loglik\"] = ll\n",
        "    out = (df.groupby([\"Beta\", \"mu_IR\"], as_index=False)\n",
        "             .agg(loglik=(\"loglik\", lambda x: logmeanexp(x.values))))\n",
        "    finite_max = out[\"loglik\"][np.isfinite(out[\"loglik\"])].max()\n",
        "    out.loc[out[\"loglik\"] < finite_max - 25, \"loglik\"] = np.nan\n",
        "    return out"
      ],
      "id": "82aef3a1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Two-dimensional likelihood slice III\n"
      ],
      "id": "be439ba1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 6,
        "fig-height": 4
      },
      "source": [
        "#| echo: false\n",
        "def plot_heatmap(df: pd.DataFrame, width=3.5, height=2.5, dpi=150):\n",
        "    piv = df.pivot(index=\"mu_IR\", columns=\"Beta\", values=\"loglik\")\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(width, height), dpi=dpi)\n",
        "    im = ax.imshow(\n",
        "        piv.values, origin=\"lower\",\n",
        "        extent=[piv.columns.min(), piv.columns.max(),\n",
        "                piv.index.min(), piv.index.max()],\n",
        "        aspect=\"auto\"\n",
        "    )\n",
        "\n",
        "    cbar = fig.colorbar(im, ax=ax, shrink=0.8)\n",
        "    ax.set_xlabel(r\"$\\beta$\", fontsize=9)\n",
        "    ax.set_ylabel(r\"$\\mu_{IR}$\", fontsize=9)\n",
        "    ax.set_title(\"Log‚Äëlikelihood surface\", fontsize=10)\n",
        "    fig.tight_layout()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    grid_thetas = make_grid()\n",
        "    loglik_vals = run_grid(grid_thetas)\n",
        "    summary_df  = summarize(grid_thetas, loglik_vals)\n",
        "    plot_heatmap(summary_df, width=6, height=4) "
      ],
      "id": "13db6614",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Two-dimensional likelihood slice IV\n",
        "\n",
        "- In the above, all points with log-likelihoods less than‚ÄØ25‚ÄØunits below the maximum are shown in grey.\n",
        "\n",
        "- Notice some features of the log-likelihood surface, and its estimate from the particle filter, that can cause difficulties for numerical methods:  \n",
        "  (a) The surface is wedge‚Äëshaped, so its curvature varies considerably. By contrast, asymptotic theory predicts a parabolic surface that has constant curvature.  \n",
        "  (b) Monte¬†Carlo noise in the likelihood evaluation makes it hard to pick out exactly where the likelihood is maximized. Nevertheless, the major features of the likelihood surface are evident despite the noise.  \n",
        "- Wedge‚Äëshaped relationships between parameters, and nonlinear relationships, are common features of epidemiological dynamic models. We‚Äôll see this in the case studies.\n",
        "\n",
        "# Exercises\n",
        "\n",
        "### Cost of a particle‚Äëfilter calculation\n",
        "\n",
        "- How much computer processing time does a particle filter take?  \n",
        "- How does this scale with the number of particles?  \n",
        "\n",
        "Form a conjecture based upon your understanding of the algorithm.  \n",
        "Test your conjecture by running a sequence of particle filter operations, with increasing numbers of particles (`Np`), measuring the time taken for each one using `system.time`.  \n",
        "Plot and interpret your results.\n",
        "\n",
        "\n",
        "### log‚Äëlikelihood estimation\n",
        "\n",
        "Here are some desiderata for a Monte¬†Carlo log‚Äëlikelihood approximation:\n",
        "\n",
        "- It should have low Monte¬†Carlo bias and variance.  \n",
        "- It should be presented together with estimates of the bias and variance so that we know the extent of Monte¬†Carlo uncertainty in our results.  \n",
        "- It should be computed in a length of time appropriate for the circumstances.  \n",
        "\n",
        "Set up a likelihood evaluation for the measles model, choosing the numbers of particles and replications so that your evaluation takes approximately one minute on your machine.\n",
        "\n",
        "- Provide a Monte¬†Carlo standard error for your estimate.  \n",
        "- Comment on the bias of your estimate.  \n",
        "\n",
        "\n",
        "### One‚Äëdimensional likelihood slice\n",
        "\n",
        "Compute several likelihood slices in the¬†$\\eta$¬†direction.\n",
        "\n",
        "### Two‚Äëdimensional likelihood slice\n",
        "\n",
        "Compute a slice of the likelihood in the¬†$\\beta$‚Äë$\\eta$¬†plane.\n",
        "\n",
        "# More on likelihood-based inference\n",
        "\n",
        "## Maximizing the likelihood\n",
        "\n",
        "### Maximizing the particle filter likelihood {.allowframebreaks}\n",
        "\n",
        "- Likelihood maximization is key to profile intervals, likelihood ratio tests and AIC as well as the computation of the MLE.  \n",
        "- An initial approach to likelihood maximization might be to stick the particle filter log‚Äëlikelihood estimate into a standard numerical optimizer, such as the Nelder‚ÄëMead algorithm.  \n",
        "- In practice this approach is unsatisfactory on all but the smallest POMP models. Standard numerical optimizers are not designed to maximize noisy and computationally expensive Monte¬†Carlo functions.  \n",
        "- Further investigation into this approach is available as a supplement.  \n",
        "- We'll present an *iterated filtering algorithm* for maximizing the likelihood in a way that takes advantage of the structure of POMP models and the particle filter.  \n",
        "- First, let's think a bit about some practical considerations in interpreting the MLE for a POMP.  \n",
        "\n",
        "### Likelihood‚Äëbased model selection and model diagnostics\n",
        "\n",
        "- For nested hypotheses, we can carry out model selection by likelihood ratio tests.  \n",
        "- For non‚Äënested hypotheses, likelihoods can be compared using Akaike's information criterion (AIC) or related methods.  \n",
        "\n",
        "## Likelihood ratio test\n",
        "\n",
        "### Likelihood ratio tests for nested hypotheses {.allowframebreaks}\n",
        "\n",
        "- The whole parameter space on which the model is defined is $\\Theta\\subset\\R^D$.  \n",
        "- Suppose we have two **nested** hypotheses  \n",
        "    \\begin{equation*}\n",
        "      \\begin{aligned}\n",
        "        H^{\\langle 0\\rangle} &: \\theta\\in \\Theta^{\\langle 0\\rangle}, \\\\\n",
        "        H^{\\langle 1\\rangle} &: \\theta\\in \\Theta^{\\langle 1\\rangle},\n",
        "      \\end{aligned}\n",
        "    \\end{equation*}\n",
        "  defined via two nested parameter subspaces, $\\Theta^{\\langle 0\\rangle}\\subset \\Theta^{\\langle 1\\rangle}$, with respective dimensions $D^{\\langle 0\\rangle}< D^{\\langle 1\\rangle}\\le D$.  \n",
        "- We consider the log‚Äëlikelihood maximized over each of the hypotheses,  \n",
        "    \\begin{equation*}\n",
        "      \\begin{aligned}\n",
        "        \\ell^{\\langle 0\\rangle} &= \\sup_{\\theta\\in \\Theta^{\\langle 0\\rangle}} \\ell(\\theta), \\\\\n",
        "        \\ell^{\\langle 1\\rangle} &= \\sup_{\\theta\\in \\Theta^{\\langle 1\\rangle}} \\ell(\\theta).\n",
        "      \\end{aligned}\n",
        "    \\end{equation*}\n",
        "- A useful approximation asserts that, under the hypothesis $H^{\\langle 0\\rangle}$,  \n",
        "    \\begin{equation*}\n",
        "      \\ell^{\\langle 1\\rangle} - \\ell^{\\langle 0\\rangle} \\approx \\tfrac{1}{2}\\,\\chi^2_{D^{\\langle 1\\rangle}- D^{\\langle 0\\rangle}},\n",
        "    \\end{equation*}\n",
        "  where $\\chi^2_d$ is a chi‚Äësquared random variable on $d$ degrees of freedom and $\\approx$ means ‚Äúis approximately distributed as‚Äù.  \n",
        "- We will call this the **Wilks approximation**.  \n",
        "- The Wilks approximation can be used to construct a hypothesis test of the null hypothesis $H^{\\langle 0\\rangle}$ against the alternative $H^{\\langle 1\\rangle}$.  \n",
        "- This is called a **likelihood ratio test** since a difference of log‚Äëlikelihoods corresponds to a ratio of likelihoods.  \n",
        "- When the data are IID, $N\\to\\infty$, and the hypotheses satisfy suitable regularity conditions, this approximation can be derived mathematically and is known as **Wilks' theorem**.  \n",
        "- The chi‚Äësquared approximation to the likelihood ratio statistic may be useful, and can be assessed empirically by a simulation study, even in situations that do not formally satisfy any known theorem.  \n",
        "\n",
        "### Wilks' theorem and profile likelihood {.allowframebreaks}\n",
        "\n",
        "- Suppose we have an MLE, written $\\hat\\theta=(\\hat\\phi,\\hat\\psi)$, and a profile log‚Äëlikelihood for $\\phi$, given by $\\profileloglik{{}}(\\phi)$.  \n",
        "- Consider the likelihood ratio test for the nested hypotheses  \n",
        "    \\begin{equation*}\n",
        "      \\begin{aligned}\n",
        "        H^{\\langle 0\\rangle} &: \\phi = \\phi_0, \\\\\n",
        "        H^{\\langle 1\\rangle} &: \\text{$\\phi$ unconstrained}.\n",
        "      \\end{aligned}\n",
        "    \\end{equation*}\n",
        "- We can compute the 95‚ÄØ%-ile for a chi‚Äësquared distribution with one degree of freedom: \\code{qchisq(0.95,df=1)}$=\\Sexpr{mysignif(qchisq(0.95,df=1),4)}$.  \n",
        "- Wilks' theorem then gives us a hypothesis test with approximate size 5‚ÄØ% that rejects $H^{\\langle 0\\rangle}$ if $\\profileloglik{{}}(\\hat\\phi)-\\profileloglik{{}}(\\phi_0)<3.84/2$.  \n",
        "- It follows that, with probability 95‚ÄØ%, the true value of $\\phi$ falls in the set  \n",
        "   \\begin{equation*}\n",
        "      \\big\\{\\phi: \\profileloglik{{}}(\\hat\\phi)-\\profileloglik{{}}(\\phi)<1.92\\big\\}.\n",
        "    \\end{equation*} \n",
        "  So, we have constructed a profile likelihood confidence interval, consisting of the set of points on the profile likelihood within 1.92‚ÄØlog units of the maximum.  \n",
        "\n",
        "## Information criteria\n",
        "\n",
        "### Akaike's information criterion (AIC) {.allowframebreaks}\n",
        "\n",
        "- Likelihood ratio tests provide an approach to model selection for nested hypotheses, but what do we do when models are not nested?  \n",
        "- A more general approach is to compare likelihoods of different models by penalizing the likelihood of each model by a measure of its complexity.  \n",
        "- Akaike's information criterion **AIC** is given by  \n",
        "    \\begin{equation*}\n",
        "      \\mathrm{AIC} = -2\\,\\loglik(\\hat{\\theta}) + 2\\,D\n",
        "    \\end{equation*} \n",
        "  ‚ÄúMinus twice the maximized log‚Äëlikelihood plus twice the number of parameters.‚Äù  \n",
        "- We are invited to select the model with the lowest AIC score.  \n",
        "- AIC was derived as an approach to minimizing prediction error. Increasing the number of parameters leads to additional **overfitting** which can decrease predictive skill of the fitted model.  \n",
        "- Viewed as a hypothesis test, AIC may have weak statistical properties. It can be a mistake to interpret AIC by making a claim that the favored model has been shown to provide a superior explanation of the data. However, viewed as a way to select a model with reasonable predictive skill from a range of possibilities, it is often useful.  \n",
        "- AIC does not penalize model complexity beyond the consequence of reduced predictive skill due to overfitting. One can penalize complexity by incorporating a more severe penalty than the $2D$ term above, such as via BIC.\n",
        "- A practical approach is to use AIC, while taking care to view it as a procedure to select a reasonable predictive model and not as a formal hypothesis test.  \n"
      ],
      "id": "e62fe1d5"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/chenj/git/pypomp/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}